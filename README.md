# Lip Reading using Deep Learning

## ğŸ“Œ Project Overview

This project aims to develop a deep learning-based lip reading system that can recognize spoken words or phrases from silent video sequences of lip movements. Lip reading, also known as visual speech recognition (VSR), is the process of understanding speech by visually interpreting the movements of the lips, face, and tongue.

The project extracts the region of interest (ROI) around the speaker's mouth, processes the video frames, and uses a combination of convolutional and recurrent neural networks to classify the spoken word or sentence.

---

## ğŸš€ Features

- ğŸ“¹ Extracts lip region from video sequences.
- ğŸ§  Uses CNN-RNN architecture (e.g., 3D CNN + BiLSTM).
- ğŸ—£ï¸ Predicts words or short sentences from silent lip movements.
- ğŸ“Š Displays accuracy, loss, and confusion matrix.
- ğŸ“ˆ Supports training, testing, and evaluation modes.
- ğŸ§ª Easily pluggable with new datasets.
- ğŸ› ï¸ Configurable architecture via a simple config file.

---

## ğŸ” How it Works

1. **Preprocessing**
   - Videos are converted to grayscale.
   - Face and mouth ROIs are extracted using tools like Dlib or OpenCV.
   - All video clips are resized and normalized.

2. **Model Architecture**
   - **Frontend:** 3D CNN (spatiotemporal feature extraction).
   - **Backend:** BiLSTM or GRU (temporal modeling).
   - **Output Layer:** Softmax or CTC decoder depending on task (classification vs sequence prediction).

3. **Training**
   - Cross-entropy or CTC loss is used.
   - Model is trained on word- or sentence-level datasets.

4. **Inference**
   - The trained model predicts the most likely word or sentence based on lip movement.

---

## ğŸ§ª Datasets Used

- **GRID Corpus**  
  [https://spandh.dcs.shef.ac.uk/gridcorpus/](https://spandh.dcs.shef.ac.uk/gridcorpus/)  
  - 34 speakers, 1,000 utterances each
  - Fixed grammar: 6-word sentences

- **Lip Reading in the Wild (LRW)**  
  [https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)  
  - Over 500 words, in-the-wild, real-world variability

- **LRS2 (Lip Reading Sentences)**  
  [https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html)  
  - Sentence-level dataset from BBC broadcasts

---

## ğŸ§  Research Papers Referenced

### Core References:
- **LipNet: End-to-End Sentence-level Lipreading**  
  Assael et al., 2016  
  [https://arxiv.org/abs/1611.01599](https://arxiv.org/abs/1611.01599)

- **Watch, Listen, Attend and Spell**  
  Afouras et al., 2018  
  [https://arxiv.org/abs/1804.03619](https://arxiv.org/abs/1804.03619)

- **Deep Lip Reading: A Comparison of Models and Datasets**  
  Wand et al., 2016  
  [https://arxiv.org/abs/1601.08188](https://arxiv.org/abs/1601.08188)

---

## ğŸ› ï¸ Technologies Used

- **Python**
- **OpenCV**, **Dlib** â€“ for preprocessing and facial landmark detection
- **PyTorch** / **TensorFlow** â€“ for model building
- **NumPy**, **Pandas** â€“ data handling
- **Matplotlib**, **Seaborn** â€“ for plotting and visualization
- **Scikit-learn** â€“ evaluation metrics

---

## ğŸ“Š Results (Sample)

| Model      | Dataset | Accuracy | WER (Word Error Rate) |
|------------|---------|----------|------------------------|
| CNN-BiLSTM | GRID    | 95.2%    | 4.1%                   |
| LipNet     | GRID    | 95.0%    | 4.8%                   |
| Transformer| LRS2    | 84.3%    | 7.9%                   |

Confusion matrix and word-level accuracy plots are available in the `results/` folder.

---

## ğŸ“Œ Installation

```bash
git clone https://github.com/yourusername/lip-reading-project.git
cd lip-reading-project
pip install -r requirements.txt
